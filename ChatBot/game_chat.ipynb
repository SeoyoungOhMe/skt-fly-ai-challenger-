{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnBfLwtfYdNi"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from konlpy.tag import Komoran\n",
        "import time\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "p = []\n",
        "with open('/mnt/c/src/chatgpt/용례_게임.json', 'r') as f:\n",
        "    json_data = json.load(f)\n",
        "    for i in json_data:\n",
        "        for j in i[\"tokens\"]:\n",
        "            a = j['sub']\n",
        "            data.append(a)\n",
        "            label = j[\"facet\"]\n",
        "            if label not in p:\n",
        "                p.append(label)\n",
        "            labels.append(p.index(label))\n",
        "            #labels.append(label)\n",
        "\n",
        "df = pd.DataFrame({'Column1': data, 'Column2': labels})\n",
        "print(df.head(3))\n",
        "print(len(p))\n",
        "df.to_csv('./game_data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 모듈 임포트\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
        "from sklearn.model_selection import train_test_split\n",
        "# 데이터 읽어오기\n",
        "\n",
        "train_file = \"./game_data.csv\"\n",
        "data = pd.read_csv(train_file, delimiter=',')\n",
        "features = data['Column1'].tolist()\n",
        "labels = data['Column2'].tolist()\n",
        "\n",
        "# 단어 인덱스 시퀀스 벡터\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "word_index = tokenizer.word_index\n",
        "MAX_SEQ_LEN = 15 # 단어 시퀀스 벡터 크기\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
        "\n",
        "# 데이터셋 생성\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(padded_seqs, labels, test_size=0.1, random_state=42)\n",
        "train_features, val_features, train_labels, val_labels = train_test_split(train_features, train_labels, test_size=0.1, random_state=42)\n",
        "\n",
        "# Define datasets\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_features, train_labels)).shuffle(buffer_size=10000).batch(64)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_features, val_labels)).batch(64)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_features, test_labels)).batch(64)\n",
        "\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "dropout_prob = 0.5\n",
        "EMB_SIZE = 128\n",
        "EPOCH = 5\n",
        "VOCAB_SIZE = len(word_index) + 1 # 전체 단어 수\n",
        "# CNN 모델 정의\n",
        "input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
        "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE,\n",
        "input_length=MAX_SEQ_LEN)(input_layer)\n",
        "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
        "\n",
        "conv1 = Conv1D(filters=128, kernel_size=3, padding='valid',\n",
        "activation=tf.nn.relu)(dropout_emb)\n",
        "pool1 = GlobalMaxPool1D()(conv1)\n",
        "conv2 = Conv1D(filters=128, kernel_size=4, padding='valid',\n",
        "activation=tf.nn.relu)(dropout_emb)\n",
        "pool2 = GlobalMaxPool1D()(conv2)\n",
        "conv3 = Conv1D(filters=128, kernel_size=5, padding='valid',\n",
        "activation=tf.nn.relu)(dropout_emb)\n",
        "pool3 = GlobalMaxPool1D()(conv3)\n",
        "# 합치기\n",
        "concat = concatenate([pool1, pool2, pool3])\n",
        "\n",
        "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
        "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
        "logits = Dense(19, name='logits')(dropout_hidden)\n",
        "\n",
        "\n",
        "predictions = Dense(19, activation=tf.nn.softmax)(logits)\n",
        "\n",
        "# 모델 생성\n",
        "model = Model(inputs=input_layer, outputs=predictions)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)\n",
        "\n",
        "# 모델 평가(테스트 데이터셋 이용)\n",
        "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
        "print('Accuracy: %f' % (accuracy * 100))\n",
        "print('loss: %f' % (loss))\n",
        "\n",
        "model.save('cnn_model_prac.h5')"
      ],
      "metadata": {
        "id": "R1lG9BFLYpq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras import preprocessing\n",
        "import json\n",
        "\n",
        "\n",
        "p = []\n",
        "with open('/mnt/c/src/chatgpt/용례_게임.json', 'r') as f:\n",
        "    json_data = json.load(f)\n",
        "    for i in json_data:\n",
        "        for j in i[\"tokens\"]:\n",
        "            label = j[\"facet\"]\n",
        "            if label not in p:\n",
        "                p.append(label)\n",
        "\n",
        "\n",
        "\n",
        "# 데이터 읽어오기\n",
        "train_file = \"./game_data.csv\"\n",
        "data = pd.read_csv(train_file, delimiter=',')\n",
        "features = data['Column1'].tolist()\n",
        "labels = data['Column2'].tolist()\n",
        "a = data['Column2'].unique()\n",
        "\n",
        "\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "MAX_SEQ_LEN = 15 # 단어 시퀀스 벡터 크기\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
        "\n",
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
        "ds = ds.shuffle(len(features))\n",
        "test_ds = ds.take(2000).batch(20) # 테스트 데이터셋\n",
        "\n",
        "model = load_model('cnn_model_prac.h5')\n",
        "model.summary()\n",
        "model.evaluate(test_ds, verbose=2)\n",
        "\n",
        "print(\"단어 시퀀스 : \", corpus[10212])\n",
        "#print(\"단어 인덱스 시퀀스 : \", padded_seqs[10212])\n",
        "print(\"문장 분류(정답) : \", labels[10212], p[2])\n",
        "\n",
        "picks = [10212]\n",
        "predict = model.predict(padded_seqs[picks])\n",
        "predict_class = tf.math.argmax(predict, axis=1)\n",
        "print(type(predict_class))\n",
        "print(\"예측 점수 : \", predict)\n",
        "print(\"예측 클래스 : \", predict_class.numpy(), p[int(predict_class)])"
      ],
      "metadata": {
        "id": "NIBjfOlFYzPG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}